### ETL-pipeline

В директории лежит питон-файл, содержайщий етл-витрину.

ПО итогу собирается DAG в airflow, который будет считаться каждый день за вчера, дополняя данные в таблице.

1. Параллельно будем обрабатывать две таблицы. В feed_actions для каждого юзера посчитаем число просмотров и лайков контента. В message_actions для каждого юзера считаем, сколько он получает и отсылает сообщений, скольким людям он пишет, сколько людей пишут ему. Каждая выгрузка должна быть в отдельном таске.

2. Далее объединяем две таблицы в одну.

3. Для этой таблицы считаем все эти метрики в разрезе по полу, возрасту и ос. Делаем три разных таска на каждый срез.

4. И финальные данные со всеми метриками записываем в отдельную таблицу в ClickHouse.

5. Каждый день таблица должна дополняться новыми данными. 

Структура финальной таблицы groups_activity_nsaf(сама таблица лежит в директории):

* Дата - event_date

* Название среза - dimension

* Значение среза - dimension_value

* Число просмотров - views

* Числой лайков - likes

* Число полученных сообщений - messages_received

* Число отправленных сообщений - messages_sent

* От скольких пользователей получили сообщения - users_received

* Скольким пользователям отправили сообщение - users_sent

* Срез - это os, gender и age

Структура дага:

![image](https://user-images.githubusercontent.com/94457858/192869371-7139a536-b800-4970-b770-7475ab2e5b04.png)


